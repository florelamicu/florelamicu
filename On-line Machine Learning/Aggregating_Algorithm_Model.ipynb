{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-decee55d6b3a>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-20-decee55d6b3a>\"\u001b[1;36m, line \u001b[1;32m18\u001b[0m\n\u001b[1;33m    from python_utils import\u001b[0m\n\u001b[1;37m                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Source code https://github.com/aromanenko/AggregatingAlgorithm/blob/SimpleAggregatingAlgorithm/AggregatingAlgorithm.ipynb\n",
    "\n",
    "# Check https://github.com/kolya95/AggregatingAlgorithms\n",
    "\n",
    "# https://github.com/vincent101/AggregatingAlgorithm\n",
    "\n",
    "# https://github.com/sameetsinghsidhu/aggregating-algorithm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import statsmodels as sm\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from python_utils import qualityMACAPE, qualityRMSE, \\\n",
    "InitExponentialSmoothing, AdaptiveExponentialSmoothing, WintersExponentialSmoothing\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(x, predicted):\n",
    "    return np.abs(x - predicted)\n",
    "\n",
    "def MSE(x, predicted):\n",
    "    return (x - predicted) ** 2\n",
    "\n",
    "def exp_MAE(x, predicted):\n",
    "    return np.exp(np.abs(x - predicted))\n",
    "\n",
    "def exp_MSE(x, predicted):\n",
    "    return np.exp((x - predicted) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'TimeSeries_Data10.csv' does not exist: b'TimeSeries_Data10.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-2a5d1c084354>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Read data (time series)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TimeSeries_Data10.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdayfirst\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Dates'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Timestamp'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# sort index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'TimeSeries_Data10.csv' does not exist: b'TimeSeries_Data10.csv'"
     ]
    }
   ],
   "source": [
    "# Read data (time series)\n",
    "ts = pd.read_csv('TimeSeries_Data10.csv', sep=',', decimal='.', parse_dates=True, dayfirst=True, index_col='Dates')\n",
    "ts.index.names=['Timestamp']\n",
    "ts = ts.sort_index() # sort index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_algs_transformer(base_algs):\n",
    "    new_algs = []\n",
    "    for i in base_algs:\n",
    "        for j in ParameterGrid(i['base_alg_params']):\n",
    "            new_algs.append({'base_alg':i['base_alg'],\n",
    "                            'base_alg_params':j})\n",
    "    return new_algs\n",
    "\n",
    "def get_preds(x, model):\n",
    "    if np.isnan(x[-1]):\n",
    "        fitted = pd.DataFrame(columns=['Values'], index=x.index).asfreq('D')\n",
    "        fitted.loc[model.fittedvalues.index, 'Values'] = model.fittedvalues\n",
    "        fitted.loc[model.fittedvalues.index[-1] + timedelta(days=1), 'Values'] = model.fcastvalues[0]\n",
    "        fitted = fitted['Values']\n",
    "        \n",
    "        fcast = pd.DataFrame(index = pd.DatetimeIndex(start=(fitted.index)[-1], end=(fitted.index)[-1]+timedelta(days=1),\n",
    "                                                  freq='D')[1:], columns=['Values'])\n",
    "        fcast['Values'] = model.fcastvalues\n",
    "        fcast = fcast['Values']\n",
    "        \n",
    "        first_not_nan = fitted[fitted.isna()==False].index[0]\n",
    "        nan_indexes = fitted[first_not_nan:][fitted[first_not_nan:].isna()].index\n",
    "        for i in nan_indexes:\n",
    "            fitted[i] = fitted[i - timedelta(days=1)]\n",
    "            \n",
    "        return pd.concat([fitted, fcast])\n",
    "    \n",
    "    else:\n",
    "\n",
    "        fitted = pd.DataFrame(columns=['Values'], index=x.index).asfreq('D')\n",
    "        fitted.loc[model.fittedvalues.index, 'Values'] = model.fittedvalues\n",
    "        fitted = fitted['Values']\n",
    "\n",
    "        fcast = pd.DataFrame(index = pd.DatetimeIndex(start=(fitted.index)[-1], end=(fitted.index)[-1]+timedelta(days=1),\n",
    "                                                  freq='D')[1:], columns=['Values'])\n",
    "        fcast['Values'] = model.fcastvalues\n",
    "        fcast = fcast['Values']\n",
    "\n",
    "        first_not_nan = fitted[fitted.isna()==False].index[0]\n",
    "        nan_indexes = fitted[first_not_nan:][fitted[first_not_nan:].isna()].index\n",
    "        for i in nan_indexes:\n",
    "            fitted[i] = fitted[i - timedelta(days=1)]\n",
    "\n",
    "        return pd.concat([fitted, fcast])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating Algorithm\n",
    "Descibe parameters of AA:\n",
    "$\\beta$ - parametr of mixability\n",
    "$S(g)$ - substitution function\n",
    "$p_j$ - initial distribution of base algorithms d* Write down theoretical boundary for AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(params):\n",
    "    beta, w, ba = params['beta'], params['weights'], params['base_alg_params']\n",
    "    N = len(ba)\n",
    "    \n",
    "    if w == 'equal':\n",
    "        w = [1/N for i in range(N)]\n",
    "    else:\n",
    "        try:\n",
    "            assert len(w) == N\n",
    "        except AssertionError:\n",
    "            print('initial weights should be either \"equal\" or an array of len(base_algs)')\n",
    "    \n",
    "    try:\n",
    "        assert 0 < beta < 1\n",
    "    except AssertionError:\n",
    "        print ('beta must be set in the (0, 1) range')\n",
    "    \n",
    "    return beta, w, ba, N\n",
    "\n",
    "def avoid_overflowing(base, power_array):\n",
    "    maximum = np.max(power_array)\n",
    "    minimum = np.min(power_array)\n",
    "    \n",
    "    pmax = -np.log(base)/np.log(2) * maximum\n",
    "    pmin = -np.log(base)/np.log(2) * minimum\n",
    "    \n",
    "    if np.abs(pmax-pmin) > 2097:\n",
    "        print('Overflow is imminent. Further calculations are not advised')\n",
    "        return base ** power_array\n",
    "    power_shift = abs((51+pmin+pmax)/2)\n",
    "    power_shift = power_shift + min(0, pmin - power_shift + 1023)\n",
    "    \n",
    "    power_array = power_array - np.abs(power_shift * np.log(2) / np.log(base))\n",
    "    \n",
    "    return base ** power_array\n",
    "def update_weights(curr_weights, init_weights, new_algs, active_algs, method='ver2'):\n",
    "    if method == 'ver1':\n",
    "        return init_weights[new_algs]\n",
    "    elif method == 'ver2':\n",
    "        return init_weights[new_algs] / np.sum(init_weights[active_algs])\n",
    "    elif method == 'ver3':\n",
    "        old_algs = np.logical_and((active_algs == True), (new_algs==False))\n",
    "        return init_weights[new_algs] / np.sum(init_weights[old_algs])\n",
    "    elif method == 'ver4':\n",
    "        best_alg = curr_weights.argmax()\n",
    "        return init_weights[new_algs] * curr_weights[best_alg] / init_weights[best_alg]\n",
    "    elif method == 'ver5':\n",
    "        worst_alg = curr_weights.argmin()\n",
    "        return init_weights[new_algs] * curr_weights[worst_alg] / init_weights[worst_alg]\n",
    "    elif method == 'ver6':\n",
    "        old_algs = np.logical_and((active_algs == True), (new_algs==False))\n",
    "        return init_weights[new_algs] * np.sum(curr_weights[old_algs]) / np.sum(init_weights[old_algs])\n",
    "\n",
    "def get_weights(last_weights, beta, loss, control_overflowing=True):\n",
    "    weights = np.log(beta) * loss\n",
    "        \n",
    "    if control_overflowing:\n",
    "        res = avoid_overflowing(np.e, weights)\n",
    "    else:\n",
    "        res = np.exp(weights)\n",
    "        \n",
    "    res = res * last_weights\n",
    "    res /= np.sum(res)\n",
    "    return res\n",
    "def count_y(data, t, method='minmax'):\n",
    "    if method=='minmax':\n",
    "        return (np.min(data[:t + 1]), np.max(data[:t + 1]))\n",
    "\n",
    "def calc_g(Y, forecast, loss, weights, beta):\n",
    "    \n",
    "    g = np.sum(weights * np.power(beta, loss(Y, forecast)))\n",
    "    g = np.log(g) / np.log(beta)\n",
    "    \n",
    "    if np.isinf(g):\n",
    "        print('TTTTTT')        \n",
    "    return g\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def upper_bound(current_loss, Y1, Y2, loss_func, M = 2, K = 2):\n",
    "    if np.min(current_loss) == 0:\n",
    "        current_loss[current_loss == 0] += 0.00000001\n",
    "    if loss_func == 'MAE':\n",
    "        alpha = np.sqrt((Y2 - Y1) * K * np.log(M) / (4 * np.min(current_loss)))\n",
    "        L = (1 + alpha) * np.min(current_loss) + (1 + alpha) / alpha * (Y2 - Y1) * K / 4 * np.log(M)\n",
    "    else:\n",
    "        L = np.min(current_loss) + (np.square(Y2 - Y1))/2 * np.log(M)\n",
    "        \n",
    "    return L\n",
    "\n",
    "def substituion_function(Y1, Y2, forecast, loss, weights, beta, method='v1'):\n",
    "    gY1 = calc_g(Y1, forecast, loss, weights, beta)\n",
    "    gY2 = calc_g(Y2, forecast, loss, weights, beta)\n",
    "    \n",
    "    if np.isinf(gY1):\n",
    "        if np.isinf(gY2):\n",
    "            return ((Y1+Y2)/2)\n",
    "        else:\n",
    "            return Y2\n",
    "    elif np.isinf(gY2):\n",
    "        return Y1\n",
    "    \n",
    "    if loss == MAE:\n",
    "        if method == 'v1':\n",
    "            y = (Y2 * gY1 + Y1 * gY2) / (gY1 + gY2)\n",
    "        else:\n",
    "            c = np.log(beta) / (2 * np.log(beta/2))\n",
    "            y = c/2 * (gY1 - gY2) + (Y1 + Y2)/2\n",
    "    elif loss==MSE:\n",
    "        if method == 'v1':\n",
    "            y = (Y2 * np.sqrt(gY1) + Y1 * np.sqrt(gY2)) / (np.sqrt(gY1) + np.sqrt(gY2))\n",
    "        else:\n",
    "            y = (gY1 - gY2) / (2 * (Y2 - Y1)) + (Y1 + Y2)/2\n",
    "        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aa_composition(x, h, params, loss_function = MAE, update_method='ver2', subst_method = 'v1'):\n",
    "    '''\n",
    "    Parameters\n",
    "    x <array> - time series\n",
    "    h <integer scalar>- forecasting delay\n",
    "    params <dict> - dictionary with \n",
    "    beta <scalar in [0,1]> - mixability parameter \n",
    "    weights <array in [0,1]> - initial weights of base_algs\n",
    "    base_algs - array of <dict> with params\n",
    "        base_alg <string> - name of base algorithm\n",
    "        base_alg_params <dict> dictionary of base algorithm's params\n",
    "    '''\n",
    "    \n",
    "    # internal params of composition\n",
    "    beta, weights, base_algs, N = init_params(params)\n",
    "    initial_weights = weights\n",
    "    algs_prev_step = [False] * N\n",
    "    T = len(x)\n",
    "    \n",
    "    AA_preds = [np.NaN]*(T+h)\n",
    "    BA_preds = np.array([np.nan]*(T+h)*N).reshape(N, T+h) \n",
    "    \n",
    "    #Getting the forecasts of basic algorithms\n",
    "    for ba in range(len(base_algs)):\n",
    "        if base_algs[ba]['base_alg'] == 'SARIMAX':\n",
    "            model = SARIMAX(x, **(base_algs[ba]['base_alg_params'])).fit()\n",
    "            preds = pd.concat([model.fittedvalues, model.forecast(h)])\n",
    "        elif base_algs[ba]['base_alg'] == 'SimpleExpSmoothing':\n",
    "            model = SimpleExpSmoothing(x.dropna()).fit(**(base_algs[ba]['base_alg_params']))\n",
    "            preds = get_preds(x, model)\n",
    "\n",
    "        BA_preds[ba] = preds\n",
    "        \n",
    "    loss, prev_loss = np.zeros(N), np.zeros(N)\n",
    "    cum_loss = np.zeros(N)\n",
    "    y = 1\n",
    "    cum_ensemble_loss = 0\n",
    "    ensemble_losses, bound = [], []\n",
    "    \n",
    "    for t in range(0, T):\n",
    "        not_sleeping_algs = (np.isnan(BA_preds.T[t])==False)\n",
    "        if not_sleeping_algs.sum() == 0:\n",
    "            pass\n",
    "        new_algs = np.logical_and(algs_prev_step==False, not_sleeping_algs==True)\n",
    "        \n",
    "        if new_algs.sum() != 0 and initial_weights != weights:\n",
    "            weights[new_algs] = update_weights(weights, initial_weights, new_algs, not_sleeping_algs, method=update_method)\n",
    "        \n",
    "        if not np.isnan(x[t]):\n",
    "            #Calculating the current losses for the moment t\n",
    "            current_preds = BA_preds.transpose()[t][not_sleeping_algs]\n",
    "            current_loss = loss_function(x[t], current_preds)\n",
    "            loss[not_sleeping_algs] = current_loss\n",
    "            cum_loss[not_sleeping_algs] += current_loss\n",
    "            \n",
    "            #Getting the weights\n",
    "            weights = get_weights(weights, beta, prev_loss)\n",
    "\n",
    "            #Calculating Y1/Y2\n",
    "            Y1, Y2 = count_y(x, t)\n",
    "            \n",
    "            #Calculating the upper bound\n",
    "            ub = upper_bound(cum_loss, Y1, Y2, loss_function)\n",
    "            \n",
    "            y = substituion_function(Y1, Y2, current_preds, loss_function, weights[not_sleeping_algs], beta, method=subst_method)\n",
    "            \n",
    "            \n",
    "            ensemble_loss = loss_function(y, x[t])\n",
    "            if (not np.isnan(ensemble_loss)):\n",
    "                cum_ensemble_loss += ensemble_loss\n",
    "            bound.append(ub)\n",
    "            ensemble_losses.append(cum_ensemble_loss)\n",
    "            \n",
    "            prev_loss, loss  = loss, np.zeros(N)\n",
    "            \n",
    "        AA_preds[t+h] = y\n",
    "        algs_prev_step = not_sleeping_algs\n",
    "    return AA_preds, ensemble_losses, bound, BA_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-f04dd0384c8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# define base algorithm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m frc_ts = pd.DataFrame(index = ts.index.append(\n\u001b[0m\u001b[0;32m     14\u001b[0m     pd.date_range(ts.index[-1]+timedelta(1), ts.index[-1]+timedelta(h))), columns = ts.columns)\n\u001b[0;32m     15\u001b[0m \u001b[0mFRC_TS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ts' is not defined"
     ]
    }
   ],
   "source": [
    "# run (build forecast) Aggregating Algorithm\n",
    "\n",
    "h =1 # forecast horizon\n",
    "\n",
    "base_algs = base_algs_transformer([{'base_alg':'SARIMAX',\n",
    "                                    'base_alg_params':{'order':[(1,1,0), (1,1,1), (2,1,1)],\n",
    "                                                       'trend':['c', 't', None]}},\n",
    "                                   {'base_alg':'SimpleExpSmoothing',\n",
    "                                    'base_alg_params':{'smoothing_level':np.arange(0.1, 1.1, 0.1)}}\n",
    "                                  ])\n",
    "\n",
    "# define base algorithm\n",
    "frc_ts = pd.DataFrame(index = ts.index.append(\n",
    "    pd.date_range(ts.index[-1]+timedelta(1), ts.index[-1]+timedelta(h))), columns = ts.columns)\n",
    "FRC_TS = dict()\n",
    "\n",
    "beta = 0.9\n",
    "bound_res = ensemble_losses_res = []\n",
    "for cntr in ts.columns:\n",
    "    frc_ts[cntr], ensemble_losses, bound, test = aa_composition(ts[cntr],h, \n",
    "                                  {'beta':0.9, 'weights': 'equal', 'base_alg_params':base_algs}, \n",
    "                                  loss_function = MAE, subst_method='v1')\n",
    "    if (ensemble_losses != []):\n",
    "        bound_res = bound\n",
    "        ensemble_losses_res = ensemble_losses\n",
    "\n",
    "        \n",
    "FRC_TS['AA beta %.2f' % (beta)] = frc_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bound_res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-7664c8ee8c2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m18\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbound_res\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbound_res\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bound'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensemble_losses_res\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensemble_losses_res\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ensemble_loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bound_res' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "plt.plot(range(len(bound_res)), np.array(bound_res), label='bound')\n",
    "plt.plot(range(len(ensemble_losses_res)), np.array(ensemble_losses_res), label='ensemble_loss')\n",
    "plt.legend()\n",
    "plt.show();\n",
    "print(ensemble_losses_res[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# draw forecast of Aggregating Algorithm\n",
    "alg_name = 'AA beta 0.90'\n",
    "for col in ts.columns:\n",
    "    ts[col].plot(figsize=(15,5), color='green', label='real')\n",
    "    forecast = FRC_TS[alg_name]\n",
    "    forecast[col].plot(color='red', label=alg_name)\n",
    "    plt.title(col)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
